[{"context": "Spatial Non-parametric Bayesian Clustered Coefficients", "source": "In the field of population health research, understanding the similarities between geographical areas and quantifying their shared effects on health outcomes is crucial. In this paper, we synthesise a number of existing methods to create a new approach that specifically addresses this goal. The approach is called a Bayesian spatial Dirichlet process clustered heterogeneous regression model. This non-parametric framework allows for inference on the number of clusters and the clustering configurations, while simultaneously estimating the parameters for each cluster. We demonstrate the efficacy of the proposed algorithm using simulated data and further apply it to analyse influential factors affecting children's health development domains in Queensland. The study provides valuable insights into the contributions of regional similarities in education and demographics to health outcomes, aiding targeted interventions and policy design.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.12349}, {"context": "Sensitivity analysis with multiple treatments and multiple outcomes with\n  applications to air pollution mixtures", "source": "Understanding the health impacts of air pollution is vital in public health research. Numerous studies have estimated negative health effects of a variety of pollutants, but accurately gauging these impacts remains challenging due to the potential for unmeasured confounding bias that is ubiquitous in observational studies. In this study, we develop a framework for sensitivity analysis in settings with both multiple treatments and multiple outcomes simultaneously. This setting is of particular interest because one can identify the strength of association between the unmeasured confounders and both the treatment and outcome, under a factor confounding assumption. This provides informative bounds on the causal effect leading to partial identification regions for the effects of multivariate treatments that account for the maximum possible bias from unmeasured confounding. We also show that when negative controls are available, we are able to refine the partial identification regions substantially, and in certain cases, even identify the causal effect in the presence of unmeasured confounding. We derive partial identification regions for general estimands in this setting, and develop a novel computational approach to finding these regions.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.12252}, {"context": "Comparison of methods for analyzing environmental mixtures effects on\n  survival outcomes and application to a population-based cohort study", "source": "The estimation of the effect of environmental exposures and overall mixtures on a survival time outcome is common in environmental epidemiological studies. While advanced statistical methods are increasingly being used for mixture analyses, their applicability and performance for survival outcomes has yet to be explored. We identified readily available methods for analyzing an environmental mixture's effect on a survival outcome and assessed their performance via simulations replicating various real-life scenarios. Using prespecified criteria, we selected Bayesian Additive Regression Trees (BART), Cox Elastic Net, Cox Proportional Hazards (PH) with and without penalized splines, Gaussian Process Regression (GPR) and Multivariate Adaptive Regression Splines (MARS) to compare the bias and efficiency produced when estimating individual exposure, overall mixture, and interaction effects on a survival outcome. We illustrate the selected methods in a real-world data application. We estimated the effects of arsenic, cadmium, molybdenum, selenium, tungsten, and zinc on incidence of cardiovascular disease in American Indians using data from the Strong Heart Study (SHS). In the simulation study, there was a consistent bias-variance trade off. The more flexible models (BART, GPR and MARS) were found to be most advantageous in the presence of nonproportional hazards, where the Cox models often did not capture the true effects due to their higher bias and lower variance. In the SHS, estimates of the effect of selenium and the overall mixture indicated negative effects, but the magnitudes of the estimated effects varied across methods. In practice, we recommend evaluating if findings are consistent across methods.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.01484}, {"context": "Developing an Index of National Research Capacity", "source": "Public managers lack feedback on the effectiveness of public investments, policies, and programs instituted to build and use research capacity. Numerous reports rank countries on global performance on innovation and competitiveness, but the highly globalized data does not distinguish country contributions from global ones. We suggest improving upon global reports by removing globalized measures and combining a reliable set of national indicators into an index. We factor analyze 14 variables for 172 countries from 2013 to 2021. Two factors emerge, one for raw or core research capacity and the other indicating the wider context of governance. Analysis shows convergent validity within the two factors and divergent validity between them. Nations rank differently between capacity, governance context, and the product of the two. Ranks also vary as a function of the chosen aggregation method. Finally, as a test of the predictive validity of the capacity index, a regression analysis was implemented predicting national citation strength. Policymakers and analysts may find stronger feedback from this approach to quantifying national research strength.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2306.03981}, {"context": "A projected nonlinear state-space model for forecasting time series\n  signals", "source": "Learning and forecasting stochastic time series is essential in various scientific fields. However, despite the proposals of nonlinear filters and deep-learning methods, it remains challenging to capture nonlinear dynamics from a few noisy samples and predict future trajectories with uncertainty estimates while maintaining computational efficiency. Here, we propose a fast algorithm to learn and forecast nonlinear dynamics from noisy time series data. A key feature of the proposed model is kernel functions applied to projected lines, enabling fast and efficient capture of nonlinearities in the latent dynamics. Through empirical case studies and benchmarking, the model demonstrates its effectiveness in learning and forecasting complex nonlinear dynamics, offering a valuable tool for researchers and practitioners in time series analysis.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.13247}, {"context": "Valid Randomization Tests in Inexactly Matched Observational Studies via\n  Iterative Convex Programming", "source": "In causal inference, matching is one of the most widely used methods to mimic a randomized experiment using observational (non-experimental) data. Ideally, treated units are exactly matched with control units for the covariates so that the treatments are as-if randomly assigned within each matched set, and valid randomization tests for treatment effects can then be conducted as in a randomized experiment. However, inexact matching typically exists, especially when there are continuous or many observed covariates or when unobserved covariates exist. Previous matched observational studies routinely conducted downstream randomization tests as if matching was exact, as long as the matched datasets satisfied some prespecified balance criteria or passed some balance tests. Some recent studies showed that this routine practice could render a highly inflated type-I error rate of randomization tests, especially when the sample size is large. To handle this problem, we propose an iterative convex programming framework for randomization tests with inexactly matched datasets. Under some commonly used regularity conditions, we show that our approach can produce valid randomization tests (i.e., robustly controlling the type-I error rate) for any inexactly matched datasets, even when unobserved covariates exist. Our framework allows the incorporation of flexible machine learning models to better extract information from covariate imbalance while robustly controlling the type-I error rate.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.11216}, {"context": "Gibbs Sampler for Matrix Generalized Inverse Gaussian Distributions", "source": "Sampling from matrix generalized inverse Gaussian (MGIG) distributions is required in Markov Chain Monte Carlo (MCMC) algorithms for a variety of statistical models. However, an efficient sampling scheme for the MGIG distributions has not been fully developed. We here propose a novel blocked Gibbs sampler for the MGIG distributions, based on the Choleski decomposition. We show that the full conditionals of the diagonal and unit lower-triangular entries are univariate generalized inverse Gaussian and multivariate normal distributions, respectively. Several variants of the Metropolis-Hastings algorithm can also be considered for this problem, but we mathematically prove that the average acceptance rates become extremely low in particular scenarios. We demonstrate the computational efficiency of the proposed Gibbs sampler through simulation studies and data analysis.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2302.09707}, {"context": "Counterfactual fairness for small subgroups", "source": "While methods for measuring and correcting differential performance in risk prediction models have proliferated in recent years, most existing techniques can only be used to assess fairness across relatively large subgroups. The purpose of algorithmic fairness efforts is often to redress discrimination against groups that are both marginalized and small, so this sample size limitation often prevents existing techniques from accomplishing their main aim. We take a three-pronged approach to address the problem of quantifying fairness with small subgroups. First, we propose new estimands built on the \"counterfactual fairness\" framework that leverage information across groups. Second, we estimate these quantities using a larger volume of data than existing techniques. Finally, we propose a novel data borrowing approach to incorporate \"external data\" that lacks outcomes and predictions but contains covariate and group membership information. This less stringent requirement on the external data allows for more possibilities for external data sources. We demonstrate practical application of our estimators to a risk prediction model used by a major Midwestern health system during the COVID-19 pandemic.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2310.19988}, {"context": "Incremental Cost-Effectiveness Statistical Inference: Calculations and\n  Communications", "source": "We illustrate use of nonparametric statistical methods to compare alternative treatments for a particular disease or condition on both their relative effectiveness and their relative cost. These Incremental Cost Effectiveness (ICE) methods are based upon Bootstrapping, i.e. Resampling with Replacement from observational or clinical-trial data on individual patients. We first show how a reasonable numerical value for the \"Shadow Price of Health\" can be chosen using functions within the ICEinfer R-package when effectiveness is not measured in \"QALY\"s. We also argue that simple histograms are ideal for communicating key findings to regulators, while our more detailed graphics may well be more informative and compelling for other health-care stakeholders.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.08604}, {"context": "Covariate adjustment in randomized experiments with missing outcomes and\n  covariates", "source": "Covariate adjustment can improve precision in estimating treatment effects from randomized experiments. With fully observed data, regression adjustment and propensity score weighting are two asymptotically equivalent methods for covariate adjustment in randomized experiments. We show that this equivalence breaks down in the presence of missing outcomes, with regression adjustment no longer ensuring efficiency gain when the true outcome model is not linear in covariates. Propensity score weighting, in contrast, still guarantees efficiency over unadjusted analysis, and including more covariates in adjustment never harms asymptotic efficiency. Moreover, we establish the value of using partially observed covariates to secure additional efficiency. Based on these findings, we recommend a simple double-weighted estimator for covariate adjustment with incomplete outcomes and covariates: (i) impute all missing covariates by zero, and use the union of the completed covariates and corresponding missingness indicators to estimate the probability of treatment and the probability of having observed outcome for all units; (ii) estimate the average treatment effect by the coefficient of the treatment from the least-squares regression of the observed outcome on the treatment, where we weight each unit by the inverse of the product of these two estimated probabilities.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.10877}]