[{"context": "A flexible framework for synthesizing human activity patterns with\n  application to sequential categorical data", "source": "The ability to synthesize realistic data in a parametrizable way is valuable for a number of reasons, including privacy, missing data imputation, and evaluating the performance of statistical and computational methods. When the underlying data generating process is complex, data synthesis requires approaches that balance realism and simplicity. In this paper, we address the problem of synthesizing sequential categorical data of the type that is increasingly available from mobile applications and sensors that record participant status continuously over the course of multiple days and weeks. We propose the paired Markov Chain (paired-MC) method, a flexible framework that produces sequences that closely mimic real data while providing a straightforward mechanism for modifying characteristics of the synthesized sequences. We demonstrate the paired-MC method on two datasets, one reflecting daily human activity patterns collected via a smartphone application, and one encoding the intensities of physical activity measured by wearable accelerometers. In both settings, sequences synthesized by paired-MC better capture key characteristics of the real data than alternative approaches.", "target": "", "edits": [], "_id": 2311.05819}, {"context": "Bayesian Adaptive Selection of Variables for Function-on-Scalar\n  Regression Models", "source": "Considering the field of functional data analysis, we developed a new Bayesian method for variable selection in function-on-scalar regression (FOSR). Our approach uses latent variables, allowing an adaptive selection since it can determine the number of variables and which ones should be selected for a function-on-scalar regression model. Simulation studies show the proposed method's main properties, such as its accuracy in estimating the coefficients and high capacity to select variables correctly. Furthermore, we conducted comparative studies with the main competing methods, such as the BGLSS method as well as the group LASSO, the group MCP and the group SCAD. We also used a COVID-19 dataset and some socioeconomic data from Brazil for real data application. In short, the proposed Bayesian variable selection model is extremely competitive, showing significant predictive and selective quality.", "target": "", "edits": [], "_id": 2303.03521}, {"context": "A Comparison of Parameter Estimation Methods for Shared Frailty Models", "source": "This paper compares six different parameter estimation methods for shared frailty models via a series of simulation studies. A shared frailty model is a survival model that incorporates a random effect term, where the frailties are common or shared among individuals within specific groups. Several parameter estimation methods are available for fitting shared frailty models, such as penalized partial likelihood (PPL), expectation-maximization (EM), pseudo full likelihood (PFL), hierarchical likelihood (HL), maximum marginal likelihood (MML), and maximization penalized likelihood (MPL) algorithms. These estimation methods are implemented in various R packages, providing researchers with various options for analyzing clustered survival data using shared frailty models. However, there is a limited amount of research comparing the performance of these parameter estimation methods for fitting shared frailty models. Consequently, it can be challenging for users to determine the most appropriate method for analyzing clustered survival data. To address this gap, this paper aims to conduct a series of simulation studies to compare the performance of different parameter estimation methods implemented in R packages. We will evaluate several key aspects, including parameter estimation, bias and variance of the parameter estimates, rate of convergence, and computational time required by each package. Through this systematic evaluation, our goal is to provide a comprehensive understanding of the advantages and limitations associated with each estimation method.", "target": "", "edits": [], "_id": 2311.11543}, {"context": "The Spatial Kernel Predictor based on Huge Observation Sets", "source": "Spatial prediction in an arbitrary location, based on a spatial set of observations, is usually performed by Kriging, being the best linear unbiased predictor (BLUP) in a least-square sense. In order to predict a continuous surface over a spatial domain a grid representation is most often used. Kriging predictions and prediction variances are computed in the nodes of a grid covering the spatial domain, and the continuous surface is assessed from this grid representation. A precise representation usually requires the number of grid nodes to be considerably larger than the number of observations. For a Gaussian random field model the Kriging predictor coinsides with the conditional expectation of the spatial variable given the observation set. An alternative expression for this conditional expectation provides a spatial predictor on functional form which does not rely on a spatial grid discretization. This functional predictor, called the Kernel predictor, is identical to the asymptotic grid infill limit of the Kriging-based grid representation, and the computational demand is primarily dependent on the number of observations - not the dimension of the spatial reference domain nor any grid discretization. We explore the potential of this Kernel predictor with associated prediction variances. The predictor is valid for Gaussian random fields with any eligible spatial correlation function, and large computational savings can be obtained by using a finite-range spatial correlation function. For studies with a huge set of observations, localized predictors must be used, and the computational advantage relative to Kriging predictors can be very large. Moreover, model parameter inference based on a huge observation set can be efficiently made. The methodology is demonstrated in a couple of examples.", "target": "", "edits": [], "_id": 2302.00354}, {"context": "Gibbs Sampler for Matrix Generalized Inverse Gaussian Distributions", "source": "Sampling from matrix generalized inverse Gaussian (MGIG) distributions is required in Markov Chain Monte Carlo (MCMC) algorithms for a variety of statistical models. However, an efficient sampling scheme for the MGIG distributions has not been fully developed. We here propose a novel blocked Gibbs sampler for the MGIG distributions, based on the Choleski decomposition. We show that the full conditionals of the diagonal and unit lower-triangular entries are univariate generalized inverse Gaussian and multivariate normal distributions, respectively. Several variants of the Metropolis-Hastings algorithm can also be considered for this problem, but we mathematically prove that the average acceptance rates become extremely low in particular scenarios. We demonstrate the computational efficiency of the proposed Gibbs sampler through simulation studies and data analysis.", "target": "", "edits": [], "_id": 2302.09707}, {"context": "On Bivariate Pseudo-Logistic Distribution: Its Properties, Estimation\n  and Applications", "source": "The literature has covered the features and uses of the traditional univariate and bivariate logistic distributions in great detail. It is reasonable to wonder, though, if logistic marginals and conditionals could exhibit a similar behavior. A phenomenon that is comparable to both bivariate exponential and bivariate normal distributions. In this study, we will concentrate on bivariate distributions where one family of conditionals is marginal and the other family is of logistic type. Pseudo-logistic distributions are the name for such distributions. Research on conditionally specified models has revealed, however, that only in cases where the variables are independent will logistic marginals and both conditionals be of the logistic form occur. We talk about the features of distributional aspects and how they are built using the original. Both the original and the new conditioning regimes are used in two different ways. Possible generalizations are also considered. We also provide an example of a Pseudo-logistic model application.", "target": "", "edits": [], "_id": 2311.08069}, {"context": "Biarchetype analysis: simultaneous learning of observations and features\n  based on extremes", "source": "A new exploratory technique called biarchetype analysis is defined. We extend archetype analysis to find the archetypes of both observations and features simultaneously. The idea of this new unsupervised machine learning tool is to represent observations and features by instances of pure types (biarchetypes) that can be easily interpreted as they are mixtures of observations and features. Furthermore, the observations and features are expressed as mixtures of the biarchetypes, which also helps understand the structure of the data. We propose an algorithm to solve biarchetype analysis. We show that biarchetype analysis offers advantages over biclustering, especially in terms of interpretability. This is because byarchetypes are extreme instances as opposed to the centroids returned by biclustering, which favors human understanding. Biarchetype analysis is applied to several machine learning problems to illustrate its usefulness.", "target": "", "edits": [], "_id": 2311.11153}, {"context": "How does international guidance for statistical practice align with the\n  ASA Ethical Guidelines?", "source": "Gillikin (2017) defines a 'practice standard' as a document to 'define the way the profession's body of knowledge is ethically translated into day-to-day activities' (Gillikin 2017, p. 1). Such documents fulfill three objectives: they 1) define the profession; 2) communicate uniform standards to stakeholders; and 3) reduce conflicts between personal and professional conduct (Gillikin, 2017 p. 2). However, there are many guidelines - this is due to different purposes that guidance writers may have, as well as to the fact that there are different audiences for the many guidance documents. The existence of diverse statements do not necessarily make it clear that there are commonalities; and while some statements are explicitly aspirational, professionals as well as the public need to know that ethically-trained practitioners follow accepted practice standards. This paper applies the methodological approach described in Tractenberg (2023) and demonstrated in Park and Tractenberg (2023) to study alignment among international guidance for official statistics, and between these guidance documents and the ASA Ethical Guidelines for Statistical Practice functioning as an ethical practice standard (Tractenberg, 2022-A, 2022-B; after Gillikin 2017). In the spirit of exchanging experiences and lessons learned, we discuss how our findings could inform closer examination, clarification, and, if beneficial, possible revision of guidance in the future.", "target": "", "edits": [], "_id": 2309.08713}, {"context": "Pairwise likelihood estimation and limited information goodness-of-fit\n  test statistics for binary factor analysis models under complex survey\n  sampling", "source": "This paper discusses estimation and limited information goodness-of-fit test statistics in factor models for binary data using pairwise likelihood estimation and sampling weights. The paper extends the applicability of pairwise likelihood estimation for factor models with binary data to accommodate complex sampling designs. Additionally, it introduces two key limited information test statistics: the Pearson chi-squared test and the Wald test. To enhance computational efficiency, the paper introduces modifications to both test statistics. The performance of the estimation and the proposed test statistics under simple random sampling and unequal probability sampling is evaluated using simulated data.", "target": "", "edits": [], "_id": 2311.02543}, {"context": "Kernel-based independence tests for causal structure learning on\n  functional data", "source": "Measurements of systems taken along a continuous functional dimension, such as time or space, are ubiquitous in many fields, from the physical and biological sciences to economics and engineering.Such measurements can be viewed as realisations of an underlying smooth process sampled over the continuum. However, traditional methods for independence testing and causal learning are not directly applicable to such data, as they do not take into account the dependence along the functional dimension. By using specifically designed kernels, we introduce statistical tests for bivariate, joint, and conditional independence for functional variables. Our method not only extends the applicability to functional data of the HSIC and its d-variate version (d-HSIC), but also allows us to introduce a test for conditional independence by defining a novel statistic for the CPT based on the HSCIC, with optimised regularisation strength estimated through an evaluation rejection rate. Our empirical results of the size and power of these tests on synthetic functional data show good performance, and we then exemplify their application to several constraint- and regression-based causal structure learning problems, including both synthetic examples and real socio-economic data.", "target": "", "edits": [], "_id": 2311.08743}, {"context": "Multivariate quantile-based permutation tests with application to\n  functional data", "source": "Permutation tests enable testing statistical hypotheses in situations when the distribution of the test statistic is complicated or not available. In some situations, the test statistic under investigation is multivariate, with the multiple testing problem being an important example. The corresponding multivariate permutation tests are then typically based on a suitableone-dimensional transformation of the vector of partial permutation p-values via so called combining functions. This paper proposes a new approach that utilizes the optimal measure transportation concept. The final single p-value is computed from the empirical center-outward distribution function of the permuted multivariate test statistics. This method avoids computation of the partial p-values and it is easy to be implemented. In addition, it allows to compute and interpret contributions of the components of the multivariate test statistic to the non-conformity score and to the rejection of the null hypothesis. Apart from this method, the measure transportation is applied also to the vector of partial p-values as an alternative to the classical combining functions. Both techniques are compared with the standard approaches using various practical examples in a Monte Carlo study. An application on a functional data set is provided as well.", "target": "", "edits": [], "_id": 2311.04017}, {"context": "Online multiple testing with e-values", "source": "A scientist tests a continuous stream of hypotheses over time in the course of her investigation -- she does not test a predetermined, fixed number of hypotheses. The scientist wishes to make as many discoveries as possible while ensuring the number of false discoveries is controlled -- a well recognized way for accomplishing this is to control the false discovery rate (FDR). Prior methods for FDR control in the online setting have focused on formulating algorithms when specific dependency structures are assumed to exist between the test statistics of each hypothesis. However, in practice, these dependencies often cannot be known beforehand or tested after the fact. Our algorithm, e-LOND, provides FDR control under arbitrary, possibly unknown, dependence. We show that our method is more powerful than existing approaches to this problem through simulations. We also formulate extensions of this algorithm to utilize randomization for increased power, and for constructing confidence intervals in online selective inference.", "target": "", "edits": [], "_id": 2311.06412}, {"context": "Factor-guided estimation of large covariance matrix function with\n  conditional functional sparsity", "source": "This paper addresses the fundamental task of estimating covariance matrix functions for high-dimensional functional data/functional time series. We consider two functional factor structures encompassing either functional factors with scalar loadings or scalar factors with functional loadings, and postulate functional sparsity on the covariance of idiosyncratic errors after taking out the common unobserved factors. To facilitate estimation, we rely on the spiked matrix model and its functional generalization, and derive some novel asymptotic identifiability results, based on which we develop DIGIT and FPOET estimators under two functional factor models, respectively. Both estimators involve performing associated eigenanalysis to estimate the covariance of common components, followed by adaptive functional thresholding applied to the residual covariance. We also develop functional information criteria for the purpose of model selection. The convergence rates of estimated factors, loadings, and conditional sparse covariance matrix functions under various functional matrix norms, are respectively established for DIGIT and FPOET estimators. Numerical studies including extensive simulations and two real data applications on mortality rates and functional portfolio allocation are conducted to examine the finite-sample performance of the proposed methodology.", "target": "", "edits": [], "_id": 2311.0245}, {"context": "Bayesian Boundary Trend Filtering", "source": "Estimating boundary curves has many applications such as economics, climate science, and medicine. Bayesian trend filtering has been developed as one of locally adaptive smoothing methods to estimate the non-stationary trend of data. This paper develops a Bayesian trend filtering for estimating the boundary trend. To this end, the truncated multivariate normal working likelihood and global-local shrinkage priors based on the scale mixtures of normal distribution are introduced. In particular, well-known horseshoe prior for difference leads to locally adaptive shrinkage estimation for boundary trend. However, the full conditional distributions of the Gibbs sampler involve high-dimensional truncated multivariate normal distribution. To overcome the difficulty of sampling, an approximation of truncated multivariate normal distribution is employed. Using the approximation, the proposed models lead to an efficient Gibbs sampling algorithm via the P\\'olya-Gamma data augmentation. The proposed method is also extended by considering a nearly isotonic constraint. The performance of the proposed method is illustrated through some numerical experiments and real data examples.", "target": "", "edits": [], "_id": 2304.11491}, {"context": "Spatial prediction of diameter distributions for the alpine protection\n  forests in Ebensee, Austria, using ALS/PLS and spatial distributional\n  regression models", "source": "A spatial distributional regression model is presented to predict the forest structural diversity in terms of the distributions of the stem diameter at breast height (DBH) in the protection forests in Ebensee, Austria. In total 36,338 sample trees were measured via a handheld mobile personal laser scanning system (PLS) on 273 sample plots each having a 20 m radius. Recent airborne laser scanning (ALS) data was used to derive regression covariates from the normalized digital vegetation height model (DVHM) and the digital terrain model (DTM). Candidate models were constructed that differed in their linear predictors of the two gamma distribution parameters. Non-linear smoothing splines outperformed linear parametric slope coefficients, and the best implementation of spatial structured effects was achieved by a Gaussian process smooth. Model fitting and posterior parameter inference was achieved by using full Bayesian methodology and MCMC sampling algorithms implemented in the R-package BAMLSS. Spatial predictions of stem count proportions per DBH classes revealed that regeneration of smaller trees was lacking in certain areas of the protection forest landscape.", "target": "", "edits": [], "_id": 2311.01893}, {"context": "Generalized Linear Models via the Lasso: To Scale or Not to Scale?", "source": "The Lasso regression is a popular regularization method for feature selection in statistics. Prior to computing the Lasso estimator in both linear and generalized linear models, it is common to conduct a preliminary rescaling of the feature matrix to ensure that all the features are standardized. Without this standardization, it is argued, the Lasso estimate will unfortunately depend on the units used to measure the features. We propose a new type of iterative rescaling of the features in the context of generalized linear models. Whilst existing Lasso algorithms perform a single scaling as a preprocessing step, the proposed rescaling is applied iteratively throughout the Lasso computation until convergence. We provide numerical examples, with both real and simulated data, illustrating that the proposed iterative rescaling can significantly improve the statistical performance of the Lasso estimator without incurring any significant additional computational cost.", "target": "", "edits": [], "_id": 2311.11236}, {"context": "Towards Sensitivity Analysis: A Workflow", "source": "Establishing causal claims is one of the primary endeavors in sociological research. Statistical causal inference is a promising way to achieve this through the potential outcome framework or structural causal models, which are based on a set of identification assumptions. However, identification assumptions are often not fully discussed in practice, which harms the validity of causal claims. In this article, we focus on the unmeasurededness assumption that assumes no unmeasured confounders in models, which is often violated in practice. This article reviews a set of papers in two leading sociological journals to check the practice of causal inference and relevant identification assumptions, indicating the lack of discussion on sensitivity analysis methods on unconfoundedness in practice. And then, a blueprint of how to conduct sensitivity analysis methods on unconfoundedness is built, including six steps of proper choices on practices of sensitivity analysis to evaluate the impacts of unmeasured confounders.", "target": "", "edits": [], "_id": 2311.1341}, {"context": "The Link Between Health Insurance Coverage and Citizenship Among\n  Immigrants: Bayesian Unit-Level Regression Modeling of Categorical Survey\n  Data Observed with Measurement Error", "source": "Social scientists are interested in studying the impact that citizenship status has on health insurance coverage among immigrants in the United States. This can be done using data from the Survey of Income and Program Participation (SIPP); however, two primary challenges emerge. First, statistical models must account for the survey design in some fashion to reduce the risk of bias due to informative sampling. Second, it has been observed that survey respondents misreport citizenship status at nontrivial rates. This too can induce bias within a statistical model. Thus, we propose the use of a weighted pseudo-likelihood mixture of categorical distributions, where the mixture component is determined by the latent true response variable, in order to model the misreported data. We illustrate through an empirical simulation study that this approach can mitigate the two sources of bias attributable to the sample design and misreporting. Importantly, our misreporting model can be further used as a component in a deeper hierarchical model. With this in mind, we conduct an analysis of the relationship between health insurance coverage and citizenship status using data from the SIPP.", "target": "", "edits": [], "_id": 2311.07524}, {"context": "Exploratory functional data analysis of multivariate densities for the\n  identification of agricultural soil contamination by risk elements", "source": "Geochemical mapping of risk element concentrations in soils is performed in countries around the world. It results in large datasets of high analytical quality, which can be used to identify soils that violate individual legislative limits for safe food production. However, there is a lack of advanced data mining tools that would be suitable for sensitive exploratory data analysis of big data while respecting the natural variability of soil composition. To distinguish anthropogenic contamination from natural variation, the analysis of the entire data distributions for smaller sub-areas is key. In this article, we propose a new data mining method for geochemical mapping data based on functional data analysis of probability density functions in the framework of Bayes spaces after post-stratification of a big dataset to smaller districts. Proposed tools allow us to analyse the entire distribution, going beyond a superficial detection of extreme concentration anomalies. We illustrate the proposed methodology on a dataset gathered according to the Czech national legislation (1990--2009). Taking into account specific properties of probability density functions and recent results for orthogonal decomposition of multivariate densities enabled us to reveal real contamination patterns that were so far only suspected in Czech agricultural soils. We process the above Czech soil composition dataset by first compartmentalising it into spatial units, in particular the districts, and by subsequently clustering these districts according to diagnostic features of their uni- and multivariate distributions at high concentration ends. Comparison between compartments is key to the reliable distinction of diffuse contamination. In this work, we used soil contamination by Cu-bearing pesticides as an example for empirical testing of the proposed data mining approach.", "target": "", "edits": [], "_id": 2310.13761}, {"context": "Posterior accuracy and calibration under misspecification in Bayesian\n  generalized linear models", "source": "Generalized linear models (GLMs) are popular for data-analysis in almost all quantitative sciences, but the choice of likelihood family and link function is often difficult. This motivates the search for likelihoods and links that minimize the impact of potential misspecification. We perform a large-scale simulation study on double-bounded and lower-bounded response data where we systematically vary both true and assumed likelihoods and links. In contrast to previous studies, we also study posterior calibration and uncertainty metrics in addition to point-estimate accuracy. Our results indicate that certain likelihoods and links can be remarkably robust to misspecification, performing almost on par with their respective true counterparts. Additionally, normal likelihood models with identity link (i.e., linear regression) often achieve calibration comparable to the more structurally faithful alternatives, at least in the studied scenarios. On the basis of our findings, we provide practical suggestions for robust likelihood and link choices in GLMs.", "target": "", "edits": [], "_id": 2311.09081}]