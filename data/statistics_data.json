[{"context": "A flexible framework for synthesizing human activity patterns with\n  application to sequential categorical data", "source": "The ability to synthesize realistic data in a parametrizable way is valuable for a number of reasons, including privacy, missing data imputation, and evaluating the performance of statistical and computational methods. When the underlying data generating process is complex, data synthesis requires approaches that balance realism and simplicity. In this paper, we address the problem of synthesizing sequential categorical data of the type that is increasingly available from mobile applications and sensors that record participant status continuously over the course of multiple days and weeks. We propose the paired Markov Chain (paired-MC) method, a flexible framework that produces sequences that closely mimic real data while providing a straightforward mechanism for modifying characteristics of the synthesized sequences. We demonstrate the paired-MC method on two datasets, one reflecting daily human activity patterns collected via a smartphone application, and one encoding the intensities of physical activity measured by wearable accelerometers. In both settings, sequences synthesized by paired-MC better capture key characteristics of the real data than alternative approaches.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.05819}, {"context": "The Link Between Health Insurance Coverage and Citizenship Among\n  Immigrants: Bayesian Unit-Level Regression Modeling of Categorical Survey\n  Data Observed with Measurement Error", "source": "Social scientists are interested in studying the impact that citizenship status has on health insurance coverage among immigrants in the United States. This can be done using data from the Survey of Income and Program Participation (SIPP); however, two primary challenges emerge. First, statistical models must account for the survey design in some fashion to reduce the risk of bias due to informative sampling. Second, it has been observed that survey respondents misreport citizenship status at nontrivial rates. This too can induce bias within a statistical model. Thus, we propose the use of a weighted pseudo-likelihood mixture of categorical distributions, where the mixture component is determined by the latent true response variable, in order to model the misreported data. We illustrate through an empirical simulation study that this approach can mitigate the two sources of bias attributable to the sample design and misreporting. Importantly, our misreporting model can be further used as a component in a deeper hierarchical model. With this in mind, we conduct an analysis of the relationship between health insurance coverage and citizenship status using data from the SIPP.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.07524}, {"context": "Generalized Linear Models via the Lasso: To Scale or Not to Scale?", "source": "The Lasso regression is a popular regularization method for feature selection in statistics. Prior to computing the Lasso estimator in both linear and generalized linear models, it is common to conduct a preliminary rescaling of the feature matrix to ensure that all the features are standardized. Without this standardization, it is argued, the Lasso estimate will unfortunately depend on the units used to measure the features. We propose a new type of iterative rescaling of the features in the context of generalized linear models. Whilst existing Lasso algorithms perform a single scaling as a preprocessing step, the proposed rescaling is applied iteratively throughout the Lasso computation until convergence. We provide numerical examples, with both real and simulated data, illustrating that the proposed iterative rescaling can significantly improve the statistical performance of the Lasso estimator without incurring any significant additional computational cost.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.11236}, {"context": "Bayesian Adaptive Selection of Variables for Function-on-Scalar\n  Regression Models", "source": "Considering the field of functional data analysis, we developed a new Bayesian method for variable selection in function-on-scalar regression (FOSR). Our approach uses latent variables, allowing an adaptive selection since it can determine the number of variables and which ones should be selected for a function-on-scalar regression model. Simulation studies show the proposed method&#x27;s main properties, such as its accuracy in estimating the coefficients and high capacity to select variables correctly. Furthermore, we conducted comparative studies with the main competing methods, such as the BGLSS method as well as the group LASSO, the group MCP and the group SCAD. We also used a COVID-19 dataset and some socioeconomic data from Brazil for real data application. In short, the proposed Bayesian variable selection model is extremely competitive, showing significant predictive and selective quality.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2303.03521}, {"context": "Pairwise likelihood estimation and limited information goodness-of-fit\n  test statistics for binary factor analysis models under complex survey\n  sampling", "source": "This paper discusses estimation and limited information goodness-of-fit test statistics in factor models for binary data using pairwise likelihood estimation and sampling weights. The paper extends the applicability of pairwise likelihood estimation for factor models with binary data to accommodate complex sampling designs. Additionally, it introduces two key limited information test statistics: the Pearson chi-squared test and the Wald test. To enhance computational efficiency, the paper introduces modifications to both test statistics. The performance of the estimation and the proposed test statistics under simple random sampling and unequal probability sampling is evaluated using simulated data.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.02543}, {"context": "On Bivariate Pseudo-Logistic Distribution: Its Properties, Estimation\n  and Applications", "source": "The literature has covered the features and uses of the traditional univariate and bivariate logistic distributions in great detail. It is reasonable to wonder, though, if logistic marginals and conditionals could exhibit a similar behavior. A phenomenon that is comparable to both bivariate exponential and bivariate normal distributions. In this study, we will concentrate on bivariate distributions where one family of conditionals is marginal and the other family is of logistic type. Pseudo-logistic distributions are the name for such distributions. Research on conditionally specified models has revealed, however, that only in cases where the variables are independent will logistic marginals and both conditionals be of the logistic form occur. We talk about the features of distributional aspects and how they are built using the original. Both the original and the new conditioning regimes are used in two different ways. Possible generalizations are also considered. We also provide an example of a Pseudo-logistic model application.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.08069}, {"context": "Online multiple testing with e-values", "source": "A scientist tests a continuous stream of hypotheses over time in the course of her investigation -- she does not test a predetermined, fixed number of hypotheses. The scientist wishes to make as many discoveries as possible while ensuring the number of false discoveries is controlled -- a well recognized way for accomplishing this is to control the false discovery rate (FDR). Prior methods for FDR control in the online setting have focused on formulating algorithms when specific dependency structures are assumed to exist between the test statistics of each hypothesis. However, in practice, these dependencies often cannot be known beforehand or tested after the fact. Our algorithm, e-LOND, provides FDR control under arbitrary, possibly unknown, dependence. We show that our method is more powerful than existing approaches to this problem through simulations. We also formulate extensions of this algorithm to utilize randomization for increased power, and for constructing confidence intervals in online selective inference.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.06412}, {"context": "The Spatial Kernel Predictor based on Huge Observation Sets", "source": "Spatial prediction in an arbitrary location, based on a spatial set of observations, is usually performed by Kriging, being the best linear unbiased predictor (BLUP) in a least-square sense. In order to predict a continuous surface over a spatial domain a grid representation is most often used. Kriging predictions and prediction variances are computed in the nodes of a grid covering the spatial domain, and the continuous surface is assessed from this grid representation. A precise representation usually requires the number of grid nodes to be considerably larger than the number of observations. For a Gaussian random field model the Kriging predictor coinsides with the conditional expectation of the spatial variable given the observation set. An alternative expression for this conditional expectation provides a spatial predictor on functional form which does not rely on a spatial grid discretization. This functional predictor, called the Kernel predictor, is identical to the asymptotic grid infill limit of the Kriging-based grid representation, and the computational demand is primarily dependent on the number of observations - not the dimension of the spatial reference domain nor any grid discretization. We explore the potential of this Kernel predictor with associated prediction variances. The predictor is valid for Gaussian random fields with any eligible spatial correlation function, and large computational savings can be obtained by using a finite-range spatial correlation function. For studies with a huge set of observations, localized predictors must be used, and the computational advantage relative to Kriging predictors can be very large. Moreover, model parameter inference based on a huge observation set can be efficiently made. The methodology is demonstrated in a couple of examples.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2302.00354}, {"context": "Exploratory functional data analysis of multivariate densities for the\n  identification of agricultural soil contamination by risk elements", "source": "Geochemical mapping of risk element concentrations in soils is performed in countries around the world. It results in large datasets of high analytical quality, which can be used to identify soils that violate individual legislative limits for safe food production. However, there is a lack of advanced data mining tools that would be suitable for sensitive exploratory data analysis of big data while respecting the natural variability of soil composition. To distinguish anthropogenic contamination from natural variation, the analysis of the entire data distributions for smaller sub-areas is key. In this article, we propose a new data mining method for geochemical mapping data based on functional data analysis of probability density functions in the framework of Bayes spaces after post-stratification of a big dataset to smaller districts. Proposed tools allow us to analyse the entire distribution, going beyond a superficial detection of extreme concentration anomalies. We illustrate the proposed methodology on a dataset gathered according to the Czech national legislation (1990--2009). Taking into account specific properties of probability density functions and recent results for orthogonal decomposition of multivariate densities enabled us to reveal real contamination patterns that were so far only suspected in Czech agricultural soils. We process the above Czech soil composition dataset by first compartmentalising it into spatial units, in particular the districts, and by subsequently clustering these districts according to diagnostic features of their uni- and multivariate distributions at high concentration ends. Comparison between compartments is key to the reliable distinction of diffuse contamination. In this work, we used soil contamination by Cu-bearing pesticides as an example for empirical testing of the proposed data mining approach.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2310.13761}, {"context": "Towards Sensitivity Analysis: A Workflow", "source": "Establishing causal claims is one of the primary endeavors in sociological research. Statistical causal inference is a promising way to achieve this through the potential outcome framework or structural causal models, which are based on a set of identification assumptions. However, identification assumptions are often not fully discussed in practice, which harms the validity of causal claims. In this article, we focus on the unmeasurededness assumption that assumes no unmeasured confounders in models, which is often violated in practice. This article reviews a set of papers in two leading sociological journals to check the practice of causal inference and relevant identification assumptions, indicating the lack of discussion on sensitivity analysis methods on unconfoundedness in practice. And then, a blueprint of how to conduct sensitivity analysis methods on unconfoundedness is built, including six steps of proper choices on practices of sensitivity analysis to evaluate the impacts of unmeasured confounders.", "target": "", "edits": [{"category": "overall_comments", "id": 1, "annotation": null}], "_id": 2311.1341}]